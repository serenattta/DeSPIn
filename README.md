# Project name

## 1 Problem Description

Large language models (LLMs) have gained significant popularity, offering a more intuitive approach to human-computer interaction. One of the key challenges in robotics is converting vague, natural-language instructions into precise machine-executable commands. Artificial intelligence provides an efficient solution, enabling robots to become more adaptable and intelligent.  

To solve this problem, our aim is to develop a framework that translates human language into actionable tasks for autonomous vehicles. Thus we designed a system where a smart car can interpret instructions and execute them autonomously within predefined constraints.

## 2 What it does

* Successfully deployed the Deepseek large language model (1.5B parameter version) on a Raspberry Pi 4, enabling effective communication.
* The primary objective is to standardize ambiguous instructions by leveraging the language model. For instance, when given an input such as *"Go find the red ball and get it back,"* the model processes the command and extracts key action-oriented components, such as *"red,"* *"go,"* and *"get it back."*
* Subsequently, the program interprets the modelâ€™s output and translates it into actionable commands. Specifically, the system navigates the vehicle forward while utilizing its camera to detect the color red, facilitating task execution.

## 3 How we built it

## 4 Challenges we ran into

## 5 Accomplishments that we're proud of/What we learned

## 6 What's next

## 7 Built With
